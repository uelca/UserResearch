{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_data = pd.read_csv(\"../data/click_data.csv\")\n",
    "emotional_events = pd.read_csv(\"../data/emotional_events.csv\")\n",
    "messages_data = pd.read_csv(\"../data/messages_data.csv\")\n",
    "user_information = pd.read_csv(\"../data/user_information.csv\")\n",
    "new_features = pd.read_csv(\"../feature_Engineering/new_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This columns are required in the new_feature File\n",
    "\n",
    "#userId,task,number_interactions_with_assistant,spent_time_with_assistant (seconds),number_interactions_with_tables,spent_time_with_tables (seconds),confidence,arousal,valence,understanding,task_time (seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From message_data we are able to extract the number of interactions with the assistand and how long an user takes at a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_data.sort_values(by=[\"userId\", \"task\", \"timestamp_unix\"], inplace=True)\n",
    "messages_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "messages_data[\"number_interactions_with_assistant\"] = 0\n",
    "messages_data[\"spent_time_with_assistant (seconds)\"] = 0\n",
    "\n",
    "for (userId, task), group in messages_data.groupby(['userId', 'task']):\n",
    "    number_interactions = group.shape[0]\n",
    "    \n",
    "    first_interaction_time = group['timestamp_unix'].min()\n",
    "    last_interaction_time = group['timestamp_unix'].max()\n",
    "    \n",
    "    total_time_spent = last_interaction_time - first_interaction_time\n",
    "    \n",
    "    messages_data.loc[group.index, 'number_interactions_with_assistant'] = number_interactions\n",
    "    messages_data.loc[group.index, 'spent_time_with_assistant (seconds)'] = total_time_spent\n",
    "\n",
    "    with open('../feature_Engineering/new_features.csv', 'a') as file:\n",
    "        file.write(f\"{userId},{task},{number_interactions},{total_time_spent}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring click_data, able to extract interactions at a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_data.sort_values(by=[\"userId\", \"task\"], inplace=True)\n",
    "click_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "click_data[\"number_interactions_with_tables\"] = 0\n",
    "click_data[\"spent_time_with_tables (seconds)\"] = 0\n",
    "\n",
    "for (userId, task), group in click_data.groupby(['userId', 'task']):\n",
    "    number_interactions = group.shape[0]\n",
    "    \n",
    "    first_interaction_time = group['timestamp_unix'].min()\n",
    "    last_interaction_time = group['timestamp_unix'].max()\n",
    "    \n",
    "    total_time_spent = last_interaction_time - first_interaction_time\n",
    "    \n",
    "    click_data.loc[group.index, 'number_interactions_with_tables'] = number_interactions\n",
    "    click_data.loc[group.index, 'spent_time_with_tables (seconds)'] = total_time_spent\n",
    "    \n",
    "    rows = []\n",
    "    csv_filename = '../feature_Engineering/new_features.csv'\n",
    "    # Read existing data from the new_features file\n",
    "    with open(csv_filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Check if the row exists and update it\n",
    "    row_exists = False\n",
    "    for row_user_information in rows:\n",
    "        user_feature = int(row_user_information['userId'])\n",
    "        task_feature = int(row_user_information['task'])\n",
    "        userID = int(userId)\n",
    "        task = int(task)\n",
    "        if user_feature == userId and task_feature == task:\n",
    "            row_user_information['number_interactions_with_tables'] = number_interactions\n",
    "            row_user_information['spent_time_with_tables (seconds)'] = total_time_spent\n",
    "            row_exists = True\n",
    "            break\n",
    "\n",
    "    # If the row doesn't exist, append a new row\n",
    "    if not row_exists:\n",
    "        new_row = {\n",
    "            'userId': userId,\n",
    "            'task': task,\n",
    "            'number_interactions_with_assistant': 0,\n",
    "            'spent_time_with_assistant (seconds)': 0,\n",
    "            'number_interactions_with_tables': number_interactions,\n",
    "            'spent_time_with_tables (seconds)': total_time_spent\n",
    "        }\n",
    "        rows.append(new_row)\n",
    "\n",
    "    # Write the updated data back to the CSV file after each iteration\n",
    "    with open(csv_filename, 'w', newline='') as file:\n",
    "        fieldnames = ['userId', 'task', 'number_interactions_with_assistant', 'spent_time_with_assistant (seconds)',\n",
    "                        'number_interactions_with_tables', 'spent_time_with_tables (seconds)', 'confidence', 'arousal', 'valence', 'understanding', 'task_time (seconds)']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring emotional_events, able to extract time per task, confidence, understanding, arousal and valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/x1s0jcnx06v5nb267srbbnnm0000gn/T/ipykernel_8891/2591562298.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filtered_df_information_finding['arousal'].replace('AO07', '7', inplace=True)\n",
      "/var/folders/8v/x1s0jcnx06v5nb267srbbnnm0000gn/T/ipykernel_8891/2591562298.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_information_finding['arousal'].replace('AO07', '7', inplace=True)\n",
      "/var/folders/8v/x1s0jcnx06v5nb267srbbnnm0000gn/T/ipykernel_8891/2591562298.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filtered_df_information_finding['valence'].replace('AO07', '7', inplace=True)\n",
      "/var/folders/8v/x1s0jcnx06v5nb267srbbnnm0000gn/T/ipykernel_8891/2591562298.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_information_finding['valence'].replace('AO07', '7', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to keep only the rows where 'task_type' is 'Information_finding'\n",
    "filtered_df_information_finding = emotional_events[emotional_events['task_type'] == 'information_finding']\n",
    "filtered_df_information_finding['arousal'].replace('AO07', '7', inplace=True)\n",
    "filtered_df_information_finding['valence'].replace('AO07', '7', inplace=True)\n",
    "\n",
    "for index, row_user_information in filtered_df_information_finding.iterrows():\n",
    "    if row_user_information[['confidence', 'arousal', 'understanding', 'valence']].notnull().all():\n",
    "        userId = row_user_information['userId']\n",
    "        task = row_user_information['task']\n",
    "        confidence = int(row_user_information['confidence'])\n",
    "        arousal = int(row_user_information['arousal'])\n",
    "        understanding = int(row_user_information['understanding'])\n",
    "        valence = int(row_user_information['valence'])\n",
    "        task_time = int(row_user_information['task_time'] )\n",
    "        \n",
    "        rows = []\n",
    "        csv_filename = '../feature_Engineering/new_features.csv'\n",
    "        # Read existing data from the new_features file\n",
    "        with open(csv_filename, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)\n",
    "\n",
    "        # Check if the row exists and update it\n",
    "        row_exists = False\n",
    "        for row_user_information in rows:\n",
    "            user_feature = int(row_user_information['userId'])\n",
    "            task_feature = int(row_user_information['task'])\n",
    "            if user_feature == userId and task_feature == task:\n",
    "                row_user_information['confidence'] = confidence\n",
    "                row_user_information['arousal'] = arousal\n",
    "                row_user_information['valence'] = valence\n",
    "                row_user_information['understanding'] = understanding\n",
    "                row_user_information['task_time (seconds)'] = task_time\n",
    "                row_exists = True\n",
    "                break\n",
    "\n",
    "        # If the row doesn't exist, append a new row\n",
    "        if not row_exists:\n",
    "            new_row = {\n",
    "                'userId': userId,\n",
    "                'task': task,\n",
    "                'number_interactions_with_assistant': 0,\n",
    "                'spent_time_with_assistant (seconds)': 0,\n",
    "                'number_interactions_with_tables': 0,\n",
    "                'spent_time_with_tables (seconds)': 0,\n",
    "                'confidence': confidence,\n",
    "                'arousal': arousal,\n",
    "                'valence': valence,\n",
    "                'understanding': understanding,\n",
    "                'task_time (seconds)': task_time\n",
    "            }\n",
    "            rows.append(new_row)\n",
    "\n",
    "        # Write the updated data back to the CSV file after each iteration\n",
    "        with open(csv_filename, 'w', newline='') as file:\n",
    "            fieldnames = ['userId', 'task', 'number_interactions_with_assistant', 'spent_time_with_assistant (seconds)',\n",
    "                        'number_interactions_with_tables', 'spent_time_with_tables (seconds)', 'confidence', 'arousal', 'valence', 'understanding', 'task_time (seconds)']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Information from user_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'userId'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8v/x1s0jcnx06v5nb267srbbnnm0000gn/T/ipykernel_906/2214925833.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_information\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'education'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'assistant_usage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'experience_analysis_tools'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../feature_Engineering/merged_new_features.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/VSCode Projects/Affective User Research/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10829\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/VSCode Projects/Affective User Research/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/VSCode Projects/Affective User Research/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/VSCode Projects/Affective User Research/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/VSCode Projects/Affective User Research/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'userId'"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames based on 'id' and 'userId'\n",
    "merged_df = new_features.merge(\n",
    "    user_information[['id', 'age', 'education', 'assistant_usage', 'experience_analysis_tools']],\n",
    "    left_on='userId',\n",
    "    right_on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "merged_df.to_csv(\"../feature_Engineering/new_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
